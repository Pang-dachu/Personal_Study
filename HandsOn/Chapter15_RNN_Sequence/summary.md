# Intro

- 미래를 예측하는 것, 시계열(time Series)
- 고정 길이가 아닌 임의 길이를 가진 시퀀스를 다룰 수 있다.

- RNN이 시퀀스형 데이터를 다루는 유일한 신경망은 아니다.
    * 작은 시퀀스 : 일반적인 신경망
    * 큰 시퀀스 (오디오, 텍스트) : CNN도 잘 작동할 수 있다.

# 15.1 순환 뉴런과 순환 층 

- RNN 구조에서 뉴런은 2개의 가중치를 가진다.
    * 하나는 현 시점(t)에 대한 가중치
    * 다른 하나는 이전 시점(t-1)에 대한 가중치 
    * 가장 첫 시점은 이전 시점이 없으므로 0으로 사용


# 15.2 RNN 훈련 

- Time Step으로 네트워크를 펼치고, 역전파 사용 
-> 해당 과정을 BackPropagation through time (BPTT)

- 손실 함수에서는 일부 출력을 무시할 수 있다. 
- 각 시점에는 같은 가중치를 사용하므로 전체 레이어의 가중치가 업데이트 된다. 


# 15.3 시계열 예측하기 

- 2가지 사례에서 사용
    1. forecasting을 하는 경우
    2. imputation의 경우 




## 15.3.3 심층 RNN

- 보통 RNN 층이 쌓이는 경우 ```return_sequence=True```를 사용하여 이전 층의 출력을 입력으로 사용
- 마지막 층의 경우 예외 발생 
    1. 마지막 층도 ```return_sequence=True``` 사용, 이후 Flatten() 사용
    2. 마지막 층은 ```return_sequence=False```사용, 이후 Dense()로 바로 연결 
    

    * 이 경우는 개인적인 선택이 아니라 데이터에서 어떤 것을 뽑아내고 싶은지에 따라 다른 듯 


 ## 15.3.4 여러 타임 스텝 앞을 예측하기 

 - Sequence-to-Sequence 의 경우 ```TimeDistributed``` 층을  사용 \


# 15.4 긴 시퀀스 다루기 

- RNN 층이 깊은 네트워크가 되는 경우 , 입력의 첫 부분을 조금씩 잊어버린다. 


## 15.4.1 불안정한 그래디언트 

- Vanising, Exploding 문제가 발생할 수 있다.

- 활성화 함수는 "수렴"하는 함수를 사용해야 한다 .

- 배치 정규화
    1. 배치 정규화는 입력에 적용하는 경우 "조금" 도움이 된다.
    2. 같은 레이어의 수평 방향이 아닌 수직방향으로 적용하는 것이 "조금" 도움이 된다. 

- 층 정규화
    1. 배치 차원의 정규화가 아닌 특성 차원에 대한 정규화
    2. 입력과 은닉 상태의 선형 조합 직후에 사용 


## 15.4.2 단기 기억 문제 

- LSTM 
- 핍홀 연결
- GRU 셀 
- 1D 합성곱 층을 사용해 시퀀스 처리 
- WAVENET 