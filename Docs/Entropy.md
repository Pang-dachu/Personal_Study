# Entropy란 무엇일까?
---

## 1. Entropy

- 본래 Entropy는 열 역학에서 사용하던 개념으로 분자의 무질서함을 측정하는 척도

- 이 개념이 인공지능 분야로 넘어오면서 불순도의 측정에 사용되었는데 이때의 Entropy는 확률 분포의 불확실성 으로 표현할 수 있다.

- 우선 확률 분포의 불확실성은 확률 분포에서 어떤 값이 나올지 확신할 수 없는 상태이다.

    ex) 동전 던지기에서 앞,뒤 가 나올 확률은 각 50%로 생각하지만 실제로는 어떤 면이 나올지 확신할 수 없는 상태이기에 동전 던지기에서의 확률 분포는 불확실성을 가지고 있는 상태임.
    (실생활에서의 대부분의 경우의 확률 분포는 불확실성을 가지고 있다고 생각함)

- Entropy는 0 ~ 1 사이의 값을 가지는데 정보의 무질서함 혹은 확률 분포의 불확실성이라는 지표이므로 1에 가까울수록 무질서, 불확실하며 0에 가까울수록 반대가 된다.

- Entropy는 분류 문제에서 사용한다.
    * 머신러닝 : Tree 알고리즘이 Entropy를 이용한 Information Gain을 통해 분기를 생성하며 결정을 잘 내리는 방향으로 학습을 진행한다.

    * 딥러닝 : 손실함수로 Cross Entropy를 사용한다.

- 그렇다면 Entropy와 Cross Entropy의 차이는 ?
    * Entropy : 단일 확률 분포의 불확실성을 나타내는 지표 
    * Cross Entropy : 두 개의 확률 분포 간의 차이를 계산하는 지표 

---
## 2. Cross Entropy

* 틀릴 수 있는 정보를 통해 구한 최적의 엔트로피 값
( Uncertainty where the info can be incorrect )

- Cross Entropy의 값은 0에서 1 사이의 값이 아님

- 분류 문제에서 예측한 결과와 실제 결과 간의 차이를 측정하기 위해 사용 

- 실제 값의 분포와 예측 값의 분포의 차이를 측정하고 이 값이 작을 수록 예측한 결과가 실제 결과에 가깝다고 판단할 수 있다. 

- 따라서 딥러닝에서의 모델은 Cross Entropy의 값이 낮아지는 방향으로 학습을 진행한다.

- 그러므로 각각 layer들이 Entropy가 낮아지는 방향으로 학습한다.
    * 상위 layer보다 하위 layer가 Entropy가 낮다. 
    * 왜냐하면 Entropy가 낮아지는 방향으로 학습하기 때문
    * feature가 충분히 학습되어 Entropy가 낮아지는 방향으로 학습이 빠르게 진행되었다면 depth가 깊지 않아도 좋은 성능을 낼 수 있을 것이다. 


---
## 3. KL Divergence
[아직 여긴 미완]
- 두 개의 확률 분포가 얼마나 서로 다른지를 나타내는 지표이다.
- KL Divergence는 한 분포가 다른 분포를 대략적으로 모사하는 데 필요한 추가적인 비용을 측정한다.

- 그렇다면 예측에 대한 확률 분포 값이 실제에 대한 확률 분포 값을 모사하는데 얼마나 비용이 필요한가를 측정하는데 사용하겠네 ? 

+  어떤 이상적인 분포에 대해, 그 분포를 근사하는 다른 분포를 사용해 샘플링을 한다면 발생할 수 있는 정보 엔트로피 차이를 계산한다.\
+ 상대 엔트로피(relative entropy), 정보 획득량(information gain), 인포메이션 다이버전스(information divergence)라고도 한다.???



---
## 4. 신경망에서는 왜 분류에서만 엔트로피 손실함수를 사용할까? 

분류 문제의 경우는

- 엔트로피 손실함수는 분류 문제에서 출력층의 확률 분포와 실제값의 확률 분포 사이의 차이를 계산하여 손실을 구한다.

- 분류 문제는 정확한 클래스를 예측하기 위해서 불확실성을 최소화해야 하기 때문에 엔트로피 손실함수가 적합하다.

그런데 회귀의 경우는 

- 연속적인 값을 예측하는 것이 목적이기 때문에 불확실성을 최소화하는 것보다는 실제값과 예측값의 차이를 최소화하는 것이 더 중요하다.

- 그렇기 때문에 회귀 문제에서는 엔트로피 손실함수 대신에 주로 평균 제곱 오차(Mean Squared Error, MSE)나 평균 절대 오차(Mean Absolute Error, MAE)를 사용한다.


그럼 여기서 생각을 전환해서 분류 문제에서는 왜 MSE, MAE를 사용하지 않을까?

> 분류 문제에서 실제 값이 2이고 예측 값이 0인 경우에 두 값의 차이인 2가 잔차라고 생각하는건 의미가 없다.
라는 의견을 제시 받았다. 


---
Reference 

[Chat GPT](https://chat.openai.com/)     
[재호아빠의 연구노트](https://dhkim9108.tistory.com/m/7)     
[꾸준희](https://eehoeskrap.tistory.com/13)      
[Skywalk](https://hyunw.kim/blog/2017/10/27/KL_divergence.html) 