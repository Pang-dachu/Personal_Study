# Chapter4. 딥러닝 프로젝트 시동 걸기와 하이퍼파라미터 튜닝

## intro
<details>
<summary>Intro</summary>

* 딥러닝 개발은 경험에 의존하는 바가 크다. 

* 고려할 것들에 대한 내용은 아래와 같다.
    - 신경망 구조는 무엇으로
    - 은닉층의 갯수 설정
    - 층의 유닛과 필터의 수
    - 학습률의 설정
    - 활성화 함수 종류
    - 데이터 수집 vs 하이퍼 파라미터 튜닝

</details>

## 4.1 성능 지표 
<details>

* 모델의 평가지표에는 정확도, 재현율, 정밀도, F1-Score등이 존재한다. 
어떤 평가지표가 정답이 아닌 상황에 따라 평가지표를 취사 선택해야 한다.

* Confusion Matrix의 사용 

* F1-Score는 정밀도와 재현율의 조화평균, 서로 Trade-off의 관계 

</details>

## 4.2 BaseLine Model 선택 
<details>

* 베이스 라인 선택시 고려해야할 부분은 다음과 같다.
    - 신경망 유형 선택 (MLP, CNN, RNN ... )
    - 혹시 Object Detection 인가 ?
    - 신경망의 층수, 활성화 함수, 최적화 알고리즘의 선택
    - 드롭아웃, 배치정규화, L2 규제 유형의 모델 규제 선택 

* 전이학습 모델이 무엇을 학습했는지를 알고 있다면 내가 직면한 문제와 유사한 전이학습 모델의 사용을 고려해봄직 하다.

* AlexNet (구조 알아둘 것)
    - 5개의 합성곱층 + 2개의 전결합층 
    - 층의 깊이(필터 수) : 합성곱층 96, 256, 384, 385, 256
    - 필터 크기 : 11, 5, 3, 3
    - 은닉층 : Relu
    - 합성곱 층에서는 MaxPool 적용
    - 전결합층(출력층 이전)의 뉴런은 4096
    - 출력층 뉴런 갯수 1000개, softmax 사용 

* AlexNet, ResNet, Inception의 구조는 익혀둘것 (특히, skip connection)

</details>

## 4.3 학습 데이터 준비 
<details>

* train, valid, test 데이터의 분할에 대한 내용

* 하나의 epoch 마다 loss, acc가 출력 

* 데이터가 100만개 이상이면 valid, test 1만개 분할(데이터가 많다의 기준)

* 주의! 데이터가 같은 분포, 특징을 따르는 가? (ex, 고해상도와 저해상도)

### 4.3.2 전처리 

* 회색조 이미지 변환 
    - 이미지의 채널을 1개로 회색조 변환하면 연산량이 줄어든다. (고려)
    - 이미지 크기를 조절(입력시 동일한 shape)

* 이미지 정규화 
    - 데이터의 입력 특징을 비슷한 분포를 가지도록 하는 것(이미지의 경우 픽셀값)
    >> 실제로 픽셀의 범위가 다른 이미지가 있으니 꼭 주의할 것 

    - 정규화의 방법
        1. 값을 작게(0~1 사이의 구간으로) 설정
        2. 이미지 픽셀값의 범위를 같은 범위로 동일하게 설정

    - 픽셀 값의 평균과 표준편차, 픽셀값에서 평균을 빼고 표준편차로 나눈다.
    (정규분포를 따르면 평균이 0이다, 데이터가 0으로 수렴하는 느낌)

    - 정규화를 하는 이유는 손실함수의 사용에도 있다.
    ( 정규분포를 하지 않으면 손실함수의 대칭성이 커진다, 원형과 타원의 형태)
    >> 전역 최소점에 도달하는 속도, 시간의 차이 

* 데이터 증강

</details>

## 4.4 모델 평가 및 성능 지표 해석 
<details>

### 4.4.1 과적합의 징후

* 과적합 확인

* 베이즈 오차율 
    - 이론적으로 모델이 실현 가능한 가장 좋은 성능을 의미

</details>

## 4.5 신경망 개선 및 하이퍼파라미터 튜닝
<details>

하이퍼 파라미터 조정,  데이터 전처리, 데이터 추가 수집의 선택 근거

* 판단 기준
    1. 훈련 데이터의 기존 성능이 납득 가능한 수준인가
    2. train_acc, val_acc의 지표를 시각화하여 확인
    3. 훈련 데이터의 성능이 낮다면 과소적합으로 기존 데이터도 잘 활용하지 못하는 것
        - 데이터 추가 수집 X
        - 하이퍼파라미터 조정
        - 데이터 클렌징, Re EDA
    
* 주요 원인을 파악할 것 
    - 낮은 성능의 원인이 데이터 : 전처리 및 추가 수집
    - 낮은 성능의 원인이 알고리즘 : 하이퍼파라미터 조정 

* 파라미터와 하이퍼파라미터 
    - 파라미터 : 가중치, 편향 : Backpropa를 통해 조정된다. 
    - 하이퍼 파라미터 : 신경망의 학습 대상이 아님.
    (learning rate, batch_size, epochs, num of layer)

### 4.5.3 신경망의 하이퍼파라미터 

* 신경망 구조 
    - 은닉층 수(깊이)
    - 층의 뉴런 갯수
    - 활성화 함수의 종류 

* 학습 및 최적화
    - 학습률, 학습률의 감쇠 유형
    - 미니배치 크기 
    - 최적화 알고리즘의 종류
    - epoch 수 

* 규제 및 과적합 방지
    - L2 규제 
    - Dropout
    - 데이터 증강 

### 4.5.4 신경망 구조 

- 필요한 학습 능력을 확보할 수 있는 신경망의 값을 찾아야함

- 검증 데이터의 오차가 더 이상 개선되지 않을 때까지 은닉층의 뉴런을 추가 

- 활성화 함수는 relu 선에서 정리 가능( swish도 고려 )

</details>

## 4.6 학습 및 최적화
<details>

* 경사하강법은 오차 함수의 값이 최소가 되도록하는 최적의 가중치 찾기 알고리즘 

* 학습률은 어떻게 선택하는가?
    - val_loss가 감소한다면 학습을 계속 진행
    - 학습이 끝나도 val_loss가 계속 감소한다면 학습률이 너무 작아 수렴하지 못한 것
        1. 학습률은 고정, epoch 증가
        2. 학습률 약간 증가 학습 진행 
    - val_loss가 진동한다면 학습률이 큰 것

### 4.6.3 학습률 감쇠 및 적응형 학습 

* 고정된 학습률을 사용하지 않고 학습률을 감쇠시키는 것
    - 학습 진행중에 학습률을 변동 시키는 것

>> adam, adagrad 선에서 정리 완료

### 4.6.4 미니배치 크기

* 배치 경사 하강법
    - 장점 : 노이즈가 적고, 최소점까지 큰 보폭으로 접근
    - 단점 : 가중치 수정시에 전체 데이터 셋을 사용, 학습 속도 느림, 컴퓨팅 자원 소모량

* 확률적 경사 하강법
    - 대체로 BGD보다 좋은 성능 
    - 단점 : 한번에 데이터 하나만 처리하므로 학습 계산 속도

* 미니 배치 경사 하강법
    - BGD, SGD의 절충안
    - 훈련 데이터 몇개를 미니 배치로 분할하여 사용
    - 계산 속도 향상 및 가중치 수정 시간이 짧다.
    - 1024 이상의 미니배치 사이즈는 잘 사용하지 않는다.

</details>

## 4.7 최적화 알고리즘 
<details>

* 모멘텀을 적용한 경사하강법
    - 잘못된 방향으로의 가중치 이동 방향이 진동하는 것을 완화하는 기법
    - 기존 가중치 수정 방식에 + 속도 

* Adam 
    - Adaptive moment estimation 
    - 경사의 평균을 속도항으로 사용, 속도항이 지수적으로 감쇠된다는 차이
    - 학습 시간이 빠르다 
    >> 그냥 Adam을 사용하자! 체고체고 

* 조기종료 
    - train_loss, val_loss를 잘 보고 과대 적합 이전에 조기 종료 

</details>

## 4.8 과적합 방지 규제 기법
<details>

1. L2 규제화는 가중치를 통해 뉴런의 영향력을 억제  
2. Dropout은 일부 뉴런의 영향력을 완전히 비활성화 


* L2 규제화
    - 오차 함수에 규제화 항을 추가하는 것 
    >> 오차 함수 = 기존 오차 함수 + 규제화 항

    - 오차 함수가 0으로는 감소하지만 0이 되지는 않도록 한다.
    - 가중치가 작아지며 뉴런의 영향력이 감소하게 된다.
    - L2 적용시 과적합 해소가 안된다면 lambda 값을 증가시켜 모델의 복잡도 저하 

* Dropout
    
* 데이터 증강 
    - 모델의 특징 학습 중 대상의 원래 모습에 대한 의존도를 낮춰버린다.

</details>

## 4.9 배치 정규화
<details>

* 이미 추출된 특징을 정규화하여 은닉층에서 정규화의 도움을 받도록 하는 것 

* 공변량 시프트
    ex) 흰색 고양이를 학습했는데 입력으로 검은색 고양이가 들어오는 경우 예측 성능이 낮아질 수 있다.

    이 경우 분포를 조정하여 "고양이"라는 특징을 잡아낼 수 있도록 하는 것으로 이해함.

* 공변량 시프트와 같은 현상에서 배치정규화는 은닉층의 출력 분포의 변화를 억제시켜버림

* 각 층마다 독립적인 학습이 가능해진다.

</details>